activation,tanh,string
anneal_quantile_sampling,False,boolean
anneal_std,False,boolean
batch_size,512,integer
coefficient_scaling_exponent,2.0,float
command_string,"python /home/Desktop/studies/q_functionals_daemon/experiments_ma.py --experiment_name results/final/3predator_1prey_case2_gaussian --env_name continuous_pred_prey_3a_coop3 --is_not_gym_env --max_episode_len 50 --saving_frequency 100000 --daemon --nb_runs 3 --update_step 50 --max_episode 40000 --max_step 2000000 --policy_type gaussian --ma_type ind",string
daemon,True,boolean
daemon_fname,/home/Desktop/studies/q_functionals_daemon/results/final/3predator_1prey_case2_gaussian/ind_run_50-step-update_func-polynomial_rank-2_episode-40000_lr0001_b512_l3_n256_m500000_iter1/timestamp.log,string
entropy_regularized,0.0,float
env_name,continuous_pred_prey_3a_coop3,string
evaluation_frequency,10000,integer
experiment_filepath,/home/Desktop/studies/q_functionals_daemon/results/final/3predator_1prey_case2_gaussian/ind_run_50-step-update_func-polynomial_rank-2_episode-40000_lr0001_b512_l3_n256_m500000_iter1,string
experiment_name,results/final/3predator_1prey_case2_gaussian,string
final_layer_init_scale,1.0,float
full_experiment_name,results/final/3predator_1prey_case2_gaussian/ind_run_50-step-update_func-polynomial_rank-2_episode-40000_lr0001_b512_l3_n256_m500000_iter1,string
functional,polynomial,string
gamma,0.99,float
hyper_parameters_name,gym,string
hyperparams_dir,/home/Desktop/studies/q_functionals_daemon/results/final/3predator_1prey_case2_gaussian/ind_run_50-step-update_func-polynomial_rank-2_episode-40000_lr0001_b512_l3_n256_m500000_iter1/hyperparams,string
interaction_qstar_samples,1000,integer
is_not_gym_env,True,boolean
layer_size,256,integer
learning_rate,0.001,float
learning_starts,10000,integer
loss_type,MSELoss,string
ma_type,ind,string
max_buffer_size,500000,integer
max_episode,40000,integer
max_episode_len,50,integer
max_step,2000000,integer
minq,False,boolean
nb_agents,3,integer
nb_runs,3,integer
noise_std,0.05,float
num_layers,3,integer
num_precomputed,1000000,integer
num_to_smooth_over,20,integer
policy_parameter,5.0,float
policy_type,gaussian,string
q_optimizer,Adam,string
qstar_samples,1000,integer
quantile_sampling_percent,0.01,float
rank,2,integer
regularization_weight,0.03,float
regularize,False,boolean
reward_clip,100.0,float
sample_method,uniform,string
save_model,True,boolean
saving_frequency,100000,integer
seed,2,integer
target_network_learning_rate,0.005,float
team_reward,False,boolean
tps_scale,0.2,float
update_step,50,integer
use_precomputed_basis,False,boolean
use_quantile_sampling_bootstrapping,False,boolean
use_quantile_sampling_evaluation_interaction,False,boolean
use_quantile_sampling_training_interaction,False,boolean
use_target_policy_smoothing,False,boolean
