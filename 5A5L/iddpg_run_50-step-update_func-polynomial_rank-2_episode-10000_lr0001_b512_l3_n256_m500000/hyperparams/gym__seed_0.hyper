activation,tanh,string
anneal_quantile_sampling,False,boolean
anneal_std,False,boolean
batch_size,512,integer
coefficient_scaling_exponent,2.0,float
command_string,"python /home/Desktop/studies/q_functionals_daemon/experiments_ma_ddpg_t.py --experiment_name results/final/5agent_5landmarks_fixed --env_name q_functions_5agent_5landmarks_fixed --is_not_gym_env --max_episode_len 50 --nb_runs 5 --update_step 50 --max_episode 10000 --max_step 500000 --policy_type gaussian --ma_type iddpg",string
daemon,False,boolean
entropy_regularized,0.0,float
env_name,q_functions_5agent_5landmarks_fixed,string
evaluation_frequency,10000,integer
experiment_filepath,/home/Desktop/studies/q_functionals_daemon/results/final/5agent_5landmarks_fixed/iddpg_run_50-step-update_func-polynomial_rank-2_episode-10000_lr0001_b512_l3_n256_m500000_iter1,string
experiment_name,results/final/5agent_5landmarks_fixed,string
final_layer_init_scale,1.0,float
full_experiment_name,results/final/5agent_5landmarks_fixed/iddpg_run_50-step-update_func-polynomial_rank-2_episode-10000_lr0001_b512_l3_n256_m500000_iter1,string
functional,polynomial,string
gamma,0.99,float
hyper_parameters_name,gym,string
hyperparams_dir,/home/Desktop/studies/q_functionals_daemon/results/final/5agent_5landmarks_fixed/iddpg_run_50-step-update_func-polynomial_rank-2_episode-10000_lr0001_b512_l3_n256_m500000_iter1/hyperparams,string
interaction_qstar_samples,1000,integer
is_not_gym_env,True,boolean
layer_size,256,integer
learning_rate,0.001,float
learning_starts,10000,integer
loss_type,MSELoss,string
ma_type,iddpg,string
max_buffer_size,500000,integer
max_episode,10000,integer
max_episode_len,50,integer
max_step,500000,integer
minq,False,boolean
nb_agents,5,integer
nb_runs,5,integer
noise_std,0.05,float
num_layers,3,integer
num_precomputed,1000000,integer
num_to_smooth_over,20,integer
policy_parameter,5.0,float
policy_type,gaussian,string
q_optimizer,Adam,string
qstar_samples,1000,integer
quantile_sampling_percent,0.01,float
rank,2,integer
regularization_weight,0.03,float
regularize,False,boolean
reward_clip,100.0,float
sample_method,uniform,string
save_model,True,boolean
saving_frequency,10000,integer
seed,0,integer
target_network_learning_rate,0.005,float
team_reward,False,boolean
tps_scale,0.2,float
update_step,50,integer
use_precomputed_basis,False,boolean
use_quantile_sampling_bootstrapping,False,boolean
use_quantile_sampling_evaluation_interaction,False,boolean
use_quantile_sampling_training_interaction,False,boolean
use_target_policy_smoothing,False,boolean
