activation,tanh,string
anneal_quantile_sampling,False,boolean
anneal_std,False,boolean
batch_size,512,integer
coefficient_scaling_exponent,2.0,float
command_string,"python /home/Desktop/studies/q_functionals_daemon/walker_experiments_ma.py --experiment_name results/final/1-walker-sample10k --env_name walker --is_sisl_env --max_episode_len 500 --daemon --nb_runs 10 --max_episode 20000 --max_step 1000000 --update_step 50 --saving_frequency 10000 --policy_type gaussian --ma_type maddpg --num_layers 2 --gamma 0.99 --learning_rate 0.001 --target_network_learning_rate 0.01",string
daemon,True,boolean
daemon_fname,/home/Desktop/studies/q_functionals_daemon/results/final/1-walker-sample10k/maddpg_run_50-step-update_individual-reward_nb-walkers-1_terrain-length-30_forward-reward-5_me-len-500_episode-20000_lr0001_tlr001_b512_l2_g099_n256_m500000_iter1/timestamp.log,string
entropy_regularized,0.0,float
env_name,walker,string
evaluation_frequency,10000,integer
experiment_filepath,/home/Desktop/studies/q_functionals_daemon/results/final/1-walker-sample10k/maddpg_run_50-step-update_individual-reward_nb-walkers-1_terrain-length-30_forward-reward-5_me-len-500_episode-20000_lr0001_tlr001_b512_l2_g099_n256_m500000_iter1,string
experiment_name,results/final/1-walker-sample10k,string
final_layer_init_scale,1.0,float
forward_reward,5,integer
full_experiment_name,results/final/1-walker-sample10k/maddpg_run_50-step-update_individual-reward_nb-walkers-1_terrain-length-30_forward-reward-5_me-len-500_episode-20000_lr0001_tlr001_b512_l2_g099_n256_m500000_iter1,string
functional,polynomial,string
gamma,0.99,float
hyper_parameters_name,gym,string
hyperparams_dir,/home/Desktop/studies/q_functionals_daemon/results/final/1-walker-sample10k/maddpg_run_50-step-update_individual-reward_nb-walkers-1_terrain-length-30_forward-reward-5_me-len-500_episode-20000_lr0001_tlr001_b512_l2_g099_n256_m500000_iter1/hyperparams,string
interaction_qstar_samples,10000,integer
is_gym_env,False,boolean
is_mpe_env,False,boolean
is_sisl_env,True,boolean
layer_size,256,integer
learning_rate,0.001,float
learning_starts,10000,integer
loss_type,MSELoss,string
ma_type,maddpg,string
max_buffer_size,500000,integer
max_episode,20000,integer
max_episode_len,500,integer
max_step,1000000,integer
minq,False,boolean
n_walkers,1,integer
nb_agents,2,integer
nb_runs,10,integer
noise_std,0.05,float
num_layers,2,integer
num_precomputed,1000000,integer
num_to_smooth_over,20,integer
policy_parameter,5.0,float
policy_type,gaussian,string
q_optimizer,Adam,string
qstar_samples,10000,integer
quantile_sampling_percent,0.01,float
rank,2,integer
regularization_weight,0.03,float
regularize,False,boolean
reward_clip,100.0,float
sample_method,uniform,string
save_model,True,boolean
saving_frequency,10000,integer
seed,9,integer
target_network_learning_rate,0.01,float
team_reward,False,boolean
terrain_length,30,integer
tps_scale,0.2,float
update_step,50,integer
use_precomputed_basis,False,boolean
use_quantile_sampling_bootstrapping,False,boolean
use_quantile_sampling_evaluation_interaction,False,boolean
use_quantile_sampling_training_interaction,False,boolean
use_target_policy_smoothing,False,boolean
